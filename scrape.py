# scrape.py

import argparse
import time
from parsers import get_category_links, parse_category

def main():
    # D√©finition des options que l'utilisateur peut passer en ligne de commande
    parser = argparse.ArgumentParser(description="Book Scraper (Books to Scrape)")
    parser.add_argument("--categories", type=str, help="Liste des cat√©gories √† scraper, s√©par√©es par des virgules (ex: Travel,Poetry)")
    parser.add_argument("--max-pages", type=int, default=None, help="Nombre max de pages √† scraper par cat√©gorie (optionnel)")
    parser.add_argument("--delay", type=float, default=None, help="D√©lai entre chaque requ√™te (en secondes)")
    parser.add_argument("--outdir", type=str, default="outputs", help="Dossier de sortie pour les CSV et les images")

    args = parser.parse_args()

    # R√©cup√®re toutes les cat√©gories disponibles sur le site
    categories = get_category_links()

    # Filtre les cat√©gories choisies par l'utilisateur
    if args.categories:
        selected = [c.strip().lower() for c in args.categories.split(",")]
        categories = [(name, url) for name, url in categories if name.lower() in selected]
        if not categories:
            print(f"‚ùå Aucune cat√©gorie trouv√©e pour : {args.categories}")
            return
    else:
        print("‚ö†Ô∏è Aucune cat√©gorie sp√©cifi√©e, toutes les cat√©gories seront scrapp√©es.")

    # Lance le scraping pour chaque cat√©gorie s√©lectionn√©e
    for name, url in categories:
        print(f"\nüöÄ D√©but du scraping pour la cat√©gorie : {name}")
        parse_category(
            category_name=name,
            url=url,
            max_pages=args.max_pages,
            delay=args.delay,
            outdir=args.outdir
        )
        time.sleep(1)  # Petite pause entre les cat√©gories

if __name__ == "__main__":
    main()
